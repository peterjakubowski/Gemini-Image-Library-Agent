{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363dcdef-a170-4800-87b3-46c93c45e495",
   "metadata": {},
   "source": [
    "# Gemini Powered Digital Asset Management\n",
    "\n",
    "This notebook illustrates the beginnings of a basic digital asset management system that is powered by Google's Gemini API. The system is designed to support management of photography image libraries. In this notebook we explore importing images into a database, capturing and generating descriptive metadata, and searching the image libraryâ€“all with the help of a Gemini chat agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9c780-51ce-4812-b881-273e1d2ccbec",
   "metadata": {},
   "source": [
    "## Image Library\n",
    "\n",
    "At its core, the DAM consists of a collection of images that live on disk. A database is kept to store persistent data that is descriptive of the images in the library. Original assets are referenced by their file path on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b5d151-1382-4fef-bf99-4fade6976308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import database packages\n",
    "import fastlite\n",
    "from pydantic import BaseModel, RootModel, Field, ConfigDict\n",
    "from pydantic.dataclasses import dataclass as pydantic_dataclass\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c321189-300d-443c-bee0-304407dc792c",
   "metadata": {},
   "source": [
    "### Core Schema\n",
    "\n",
    "The root table of our database keeps track of the assets in our system. It needs few fields:\n",
    "* id: unique id for the image asset in the database\n",
    "* image_path: unique file path to the original image on disk\n",
    "* file_name: (optional) name of the image file\n",
    "* file_type: (optional) file type of the orginal image on disk\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f540353c-d948-4a46-a034-030c1a4d0764",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pydantic_dataclass\n",
    "class Assets:\n",
    "    # Configure the BaseModel to ignore any extra attributes given at creation\n",
    "    model_config = ConfigDict(extra='ignore')\n",
    "    id: int = Field(default=None)\n",
    "    image_path: str = Field(default=None)\n",
    "    file_name: str | None = Field(default=None)\n",
    "    file_type: str | None = Field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed06f3c-21f7-4d04-a303-ae42d17b8b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load/Create the database\n",
    "db = fastlite.database('image_library.db')\n",
    "# Create core table\n",
    "assets = db.t.assets\n",
    "if not assets.exists():\n",
    "    assets = db.create(Assets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee0e196-77bb-42f5-a66f-8bcd7930afd6",
   "metadata": {},
   "source": [
    "### Metadata Schema\n",
    "\n",
    "The metadata table in our database keeps record of select image metadata that is extracted from the original image file on disk. For simplicty, we're going to save our list of keywords as a comma seperated string.\n",
    "* id: unique id for the image asset in the database\n",
    "* capture_date: date the original image was created\n",
    "* description: caption or description of the image\n",
    "* keywords: a list of descriptive words or short phrases that describe the image\n",
    "* creator: name of the image creator\n",
    "* person_in_image: a list of names labeling people shown in the image\n",
    "* location: location shown or where image was originally taken\n",
    "* city: city shown or where image was originally taken\n",
    "* state: state shown or where image was originally taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ef03e2a-2f1b-4270-ac59-ee9253e2a84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata schema\n",
    "@pydantic_dataclass\n",
    "class AssetMetadata:\n",
    "    # Configure the BaseModel to ignore any extra attributes given at creation\n",
    "    model_config = ConfigDict(extra='ignore')\n",
    "    id: int = Field(default=None)\n",
    "    capture_date: datetime | None = Field(default=None)\n",
    "    description: str | None = Field(default=None)\n",
    "    keywords: str | None = Field(default=None)\n",
    "    creator: str | None = Field(default=None)\n",
    "    person_in_image: str | None = Field(default=None)                  \n",
    "    location: str | None = Field(default=None)\n",
    "    city: str | None = Field(default=None)\n",
    "    state: str | None = Field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a965f570-fa34-47c0-9a73-414c0f6470be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata table\n",
    "asset_metadata = db.t.asset_metadata\n",
    "if not asset_metadata.exists():\n",
    "    asset_metadata = db.create(AssetMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf349f7-d83c-4031-a7ae-4d2aa46b6173",
   "metadata": {},
   "source": [
    "### Generative Schema\n",
    "\n",
    "The generative table in our database keeps record of generated content created by the Gemini model. For simplicty, we're going to save our list of keywords as a comma seperated string.\n",
    "* id: unique id for the image asset in the database\n",
    "* description: generated description of the image\n",
    "* keywords: a list of generated keywords that describe the image\n",
    "* style: style of the image as genai sees it (from taxonomy)\n",
    "* mood: mood of the image as genai sees it (from taxonomy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de525704-93af-4974-be72-113180b31ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generative schema\n",
    "@pydantic_dataclass\n",
    "class GenerativeMetadata:\n",
    "    # Configure the BaseModel to ignore any extra attributes given at creation\n",
    "    model_config = ConfigDict(extra='ignore')\n",
    "    id: int = Field(default=None)\n",
    "    description: str = Field(default=None)\n",
    "    keywords: str = Field(default=None)\n",
    "    style: str = Field(default=None)\n",
    "    mood: str = Field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8eb242-ab53-4ca2-89cf-01d73cd22b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generative table\n",
    "generative_metadata = db.t.generative_metadata\n",
    "if not generative_metadata.exists():\n",
    "    generative_metadata = db.create(GenerativeMetadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a203727e-0a40-4149-ae19-f8bd4bf12220",
   "metadata": {},
   "source": [
    "### Embedding Schema\n",
    "\n",
    "The embedding table in our database keeps record of embeddings generated by the Gemini model.\n",
    "* id: unique id for the image asset in the database\n",
    "* genai_description_vector: embedding representing the generated description of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef903b31-9062-408a-b5da-f4bff006692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding schema\n",
    "@pydantic_dataclass\n",
    "class Embeddings:\n",
    "    # Configure the BaseModel to ignore any extra attributes given at creation\n",
    "    model_config = ConfigDict(extra='ignore')\n",
    "    id: int = Field(default=None)\n",
    "    genai_description_vector: bytes = Field(default=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cf3883b-c740-4946-bb2f-5ce656f0d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding table\n",
    "embeddings = db.t.embeddings\n",
    "if not embeddings.exists():\n",
    "    embeddings = db.create(Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "321c9992-7d50-4eea-8276-c1789cc20480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE [assets] (\n",
      "   [id] INTEGER PRIMARY KEY,\n",
      "   [image_path] TEXT,\n",
      "   [file_name] TEXT,\n",
      "   [file_type] TEXT\n",
      ");\n",
      "CREATE TABLE [asset_metadata] (\n",
      "   [id] INTEGER PRIMARY KEY,\n",
      "   [capture_date] TEXT,\n",
      "   [description] TEXT,\n",
      "   [keywords] TEXT,\n",
      "   [creator] TEXT,\n",
      "   [person_in_image] TEXT,\n",
      "   [location] TEXT,\n",
      "   [city] TEXT,\n",
      "   [state] TEXT\n",
      ");\n",
      "CREATE TABLE [generative_metadata] (\n",
      "   [id] INTEGER PRIMARY KEY,\n",
      "   [description] TEXT,\n",
      "   [keywords] TEXT,\n",
      "   [style] TEXT,\n",
      "   [mood] TEXT\n",
      ");\n",
      "CREATE TABLE [embeddings] (\n",
      "   [id] INTEGER PRIMARY KEY,\n",
      "   [genai_description_vector] BLOB\n",
      ");\n",
      "CREATE TABLE sqlite_stat1(tbl,idx,stat);\n",
      "CREATE TABLE sqlite_stat4(tbl,idx,neq,nlt,ndlt,sample);\n"
     ]
    }
   ],
   "source": [
    "# Our database is now set up, let's view its schema\n",
    "print(db.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7b93-4537-45be-b1f7-3296ed7b963e",
   "metadata": {},
   "source": [
    "### Gemini API Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c7c53bc-25e0-478e-97d0-f2be0451d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7819962e-3071-4a80-a226-b93f7e47edc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get our api key\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "# Set up gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65579d24-bdd2-476d-a497-b136d198e0ac",
   "metadata": {},
   "source": [
    "#### Gemini Image Descripton Content Generation\n",
    "\n",
    "We'll use Gemini as our vision model to generate new descriptions of our images. We'll pass our image to the model and ask for a description, list of keywords, style, and mood. Let's define our json respsonse schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ed9731a-2627-475e-a830-da06f6028e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model response schema\n",
    "class DescriptionResponseSchema(BaseModel):\n",
    "    description: str  # Long description of the image\n",
    "    keywords: list[str]  # List of keywords describing the image\n",
    "    style: str  # The style of the image\n",
    "    mood: str  # The mood of the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f25c0-8fff-4a53-b444-56c3958dccb8",
   "metadata": {},
   "source": [
    "Two of our fields, style and mood, will use a taxonomy. Two taxonomies were generated in the Taxonomy.ipynb and saved to json files on disk. Let's load our taxonomies so we can use them with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53718f3-eb8e-44ff-9acc-247a69e941c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load taxonomy lists\n",
    "with open('../data/style_taxonomy.json') as style_f:\n",
    "    STYLE_TAX = json.load(style_f)\n",
    "with open('../data/mood_taxonomy.json') as mood_f:\n",
    "    MOOD_TAX = json.load(mood_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe701afa-301c-4903-a4d4-acea08249ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abstract', 'Airy', 'Artistic', 'Authentic', 'Balanced', 'Bold', 'Bright', 'Candid', 'Chiaroscuro', 'Cinematic', 'Classic', 'Clean', 'Colorful', 'Contemporary', 'Contrasty', 'Cozy', 'Dark', 'Delicate', 'Detailed', 'Dramatic', 'Dreamy', 'Dynamic', 'Earthy', 'Elegant', 'Emotional', 'Energetic', 'Fine-Art', 'Flat', 'Flowing', 'Folk', 'Fragmented', 'Geometric', 'Glamorous', 'Gritty', 'High-Key', 'Illustrative', 'Intimate', 'Layered', 'Light', 'Lomo', 'Low-Key', 'Minimalist', 'Monochromatic', 'Moody', 'Natural', 'Nostalgic', 'Painterly', 'Playful', 'Pop', 'Rustic', 'Soft', 'Surreal', 'Vintage'] ['Melancholy', 'Joyful', 'Serene', 'Agitated', 'Tranquil', 'Frightened', 'Peaceful', 'Enraged', 'Content', 'Disgusted', 'Optimistic', 'Pessimistic', 'Romantic', 'Lonely', 'Playful', 'Solemn', 'Mysterious', 'Bored', 'Excited', 'Guilty', 'Empowering', 'Shameful', 'Reflective', 'Jealous', 'Dreamy', 'Betrayed', 'Vibrant', 'Nostalgic', 'Tense', 'Comforting', 'Suspicious', 'Hopeful', 'Despairing', 'Curious', 'Resentful', 'Lively', 'Grief', 'Whimsical', 'Apathetic', 'Passionate', 'Insecure', 'Radiant', 'Vulnerable', 'Intense', 'Defiant', 'Gentle', 'Sorrowful', 'Eerie', 'Triumphant', 'Yearning']\n"
     ]
    }
   ],
   "source": [
    "print(STYLE_TAX['taxonomy'], MOOD_TAX['taxonomy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61a7a19-001f-4a21-b8bc-536c8f712da0",
   "metadata": {},
   "source": [
    "Our model needs instructions, we'll list some guidelines and include the taxonomies for style and mood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea84f657-325e-48de-97f3-a5fb1be156ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_INSTRUCTIONS_FOR_DESCRIPTIONS = f\"\"\"\n",
    "        Write a caption for the images you see using The Associated Press standards for photo captions.\n",
    "        Be very descriptive and thoroughly describe the image using 100-500 words.\n",
    "        The description you write will be used for the search and retrieval of visual assets in a digital asset management system.\n",
    "        I may provide you with additional context, but not always.\n",
    "        Additional context might be a description of the image, date, location, keywords, and persons shown in the image.\n",
    "        Use all additional context that I provide you about the image to inform the description you write.\n",
    "        You must include the date when it is provided to you, otherwise do not guess the date.\n",
    "        Include the location when it is provided to you.\n",
    "        Generate a list of keywords or short descriptive phrases that relate to the image.\n",
    "        Keywords can be one to two words long and may describe the contents, feeling, style, or mood of the image.\n",
    "        Generate no more than 20 keywords.\n",
    "        Label the style and mood of the image using only keywords from the taxonomy.\n",
    "        Style taxonomy: {STYLE_TAX['taxonomy']}\n",
    "        Mood taxonomy: {MOOD_TAX['taxonomy']}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9d3f29b-0761-4c77-87b3-436c6cf46ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini model for description generation\n",
    "GEMINI_DESCRIPTION_CONFIG = types.GenerateContentConfig(safety_settings=None,\n",
    "                                                        system_instruction=SYSTEM_INSTRUCTIONS_FOR_DESCRIPTIONS,\n",
    "                                                        max_output_tokens=2048,\n",
    "                                                        temperature=0.3,\n",
    "                                                        top_p=0.6,\n",
    "                                                        top_k=32,\n",
    "                                                        presence_penalty=0.3,\n",
    "                                                        frequency_penalty=0.3,\n",
    "                                                        response_mime_type='application/json',\n",
    "                                                        response_schema=DescriptionResponseSchema\n",
    "                                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e11ab3-d468-401c-bf0c-d1dcda6a0f0f",
   "metadata": {},
   "source": [
    "We're going to set up a one-shot Gemini model for the description generation. We'll give it the image as well as the image's metadata to give it some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fa76c0-e045-45bf-98ef-167a22edea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a Gemini model for inference\n",
    "class GeminiDescription:\n",
    "    def __init__(self, _image: Image.Image, _image_metadata: AssetMetadata, _record: Assets):\n",
    "        self.image = _image\n",
    "        self.image_metadata = _image_metadata\n",
    "        self.record = _record\n",
    "        self.description = self.generate_description()\n",
    "\n",
    "    def generate_description(self) -> GenerativeMetadata:    \n",
    "        _result = client.models.generate_content(model='gemini-2.0-flash-exp',\n",
    "                                                 config=GEMINI_DESCRIPTION_CONFIG,\n",
    "                                                 contents=[self.image_metadata, self.image])\n",
    "        _result_dict = json.loads(_result.text)\n",
    "        \n",
    "        return GenerativeMetadata(id=self.record.id,\n",
    "                                  description=_result_dict['description'],\n",
    "                                  keywords=\", \".join(_result_dict['keywords']),\n",
    "                                  style=_result_dict['style'],\n",
    "                                  mood=_result_dict['mood'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da375a-5540-4090-b3ee-cccabb59caa7",
   "metadata": {},
   "source": [
    "Let's set up an additional Gemini model to generate an embedding of the generated description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "176a269b-bc6b-46c4-9450-600aeeec02ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini model for embedding\n",
    "class GeminiEmbedding:\n",
    "    def __init__(self, _description: str, _record: Assets):\n",
    "        self.description = _description\n",
    "        self.record = _record\n",
    "        self.embedding = self.generate_embedding()\n",
    "\n",
    "    def generate_embedding(self) -> Embeddings:\n",
    "        result = client.models.embed_content(model=\"models/text-embedding-004\",\n",
    "                                             contents=self.description)\n",
    "        _embedding = np.array(result.embeddings[0].values).astype(np.float32).tobytes()\n",
    "\n",
    "        return Embeddings(id=self.record.id,\n",
    "                          genai_description_vector=_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9ebfba-33bf-4034-ba4b-a7d13db83e1c",
   "metadata": {},
   "source": [
    "## Import\n",
    "\n",
    "Importing an image into the library is the first step in our interaction with the DAM. The import workflow has a number of steps that are facilitated by our Gemini agent. \n",
    "\n",
    "* First, we start a chat with our agent by providing it an image. In this workflow, we will be selecting images on disk, apposed to uploading to a platform.\n",
    "\n",
    "* Once we kick-off the workflow, our agent with create a new record in the database for the image we are currently importing. It assigns our image an unique asset id and extracts some key information that may be embedded in the file.\n",
    "\n",
    "* From there, our agent will generate new descriptive content for the image by passing it to Gemini model for vision analysis.\n",
    "\n",
    "* Finally, our agent sends the newly generated description to the Gemini embedding model to be vectorized. These embeddings will allow for semantic search of our image library.\n",
    "\n",
    "Let's start by defining some tools to use in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feff6c36-cf84-4b31-a46c-448672edad87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for import workflow\n",
    "from pillow_metadata.metadata import Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44060554-53c1-4706-9cc2-4a95d5c3efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the image from the file path\n",
    "def open_image(_image_path: str) -> Image.Image:\n",
    "    _image = Image.open(_image_path)\n",
    "    # resize the image\n",
    "    return _image\n",
    "\n",
    "# Read some metadata from the image file\n",
    "def read_metadata(_image: Image.Image, _record: Assets) -> AssetMetadata:\n",
    "    _meta = Metadata(_image)\n",
    "    _extracted = {'capture_date': _meta.metadata.xmp.CreateDate,\n",
    "                  'description': _meta.metadata.dc.description,\n",
    "                  'keywords': \", \".join(_keywords) if (_keywords := _meta.metadata.dc.subject) else _keywords,\n",
    "                  'creator': _meta.metadata.exif.Artist,\n",
    "                  'person_in_image': \", \".join(_person_in_image) if (_person_in_image := _meta.metadata.Iptc4xmpExt.PersonInImage) else _person_in_image,\n",
    "                  'location': _meta.metadata.Iptc4xmpCore.Location,\n",
    "                  'city': _meta.metadata.photoshop.City,\n",
    "                  'state': _meta.metadata.photoshop.State}\n",
    "    \n",
    "    return AssetMetadata(**_extracted, id=_record.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ba6a51b-01b2-424b-944d-812a54c3655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to help us insert and update records in the database\n",
    "def insert_asset(_asset: Assets, _update: bool) -> Assets | None:\n",
    "        \n",
    "    # check if this asset is already in the database\n",
    "    if not (query := db.q(f\"SELECT id FROM assets WHERE file_name == '{_asset.file_name}'\")):\n",
    "        _record = assets.insert(_asset)\n",
    "        \n",
    "        return Assets(**_record)\n",
    "        \n",
    "    elif _update:\n",
    "        _asset.id = query[0]['id']\n",
    "        _record = assets.update(_asset)\n",
    "        \n",
    "        return Assets(**_record)\n",
    "\n",
    "    return None\n",
    "\n",
    "def insert_metadata(_asset_metadata: AssetMetadata) -> AssetMetadata:\n",
    "    # if asset metadata is not in the database for this record, then insert it\n",
    "    if not db.q(f\"SELECT id FROM asset_metadata WHERE id == {_asset_metadata.id}\"):\n",
    "        _record = asset_metadata.insert(_asset_metadata)\n",
    "    # otherwise update the record in the database\n",
    "    else:\n",
    "        _record = asset_metadata.update(_asset_metadata)\n",
    "\n",
    "    return _record\n",
    "\n",
    "def insert_genai_description(_genai_desc: GenerativeMetadata) -> GenerativeMetadata:\n",
    "    # if genai description is not in the database for this record, then insert it\n",
    "    if not db.q(f\"SELECT id FROM generative_metadata WHERE id == {_genai_desc.id}\"):\n",
    "        _record = generative_metadata.insert(_genai_desc)\n",
    "    # otherwise update the record in the database\n",
    "    else:\n",
    "        _record = generative_metadata.update(_genai_desc)\n",
    "    \n",
    "    return _record\n",
    "\n",
    "def insert_embedding(_embedding: Embeddings) -> Embeddings:\n",
    "    # if embedding is not in the database for this record, then insert it\n",
    "    if not db.q(f\"SELECT id FROM embeddings WHERE id == {_embedding.id}\"):\n",
    "        _record = embeddings.insert(_embedding)\n",
    "    # otherwise update the record in the database\n",
    "    else:\n",
    "        _record = embeddings.update(_embedding)\n",
    "    \n",
    "    return _record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a5260-6e9b-46af-8012-775257c44a47",
   "metadata": {},
   "source": [
    "Let's go through each step of the import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26c1fa4f-e53c-443e-a69e-20084acee35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap our tools in a import workflow\n",
    "def import_image(_image_path: str, _update=False):\n",
    "    # Create an Assets object for our image\n",
    "    _asset = Assets(image_path=_image_path, file_name=_image_path.split('/')[-1])\n",
    "    # let's open the image\n",
    "    _image = open_image(_asset.image_path)\n",
    "    # check if the image already exists in the database\n",
    "    if _record := insert_asset(_asset, _update):\n",
    "        # get the image's embedded metadata\n",
    "        _img_meta = read_metadata(_image, _record)\n",
    "        # insert the metadata into the database\n",
    "        _meta_record = insert_metadata(_img_meta)\n",
    "        # generate image descriptions\n",
    "        time.sleep(1)\n",
    "        _genai_desc = GeminiDescription(_image,\n",
    "                                        RootModel[AssetMetadata](_meta_record).model_dump_json(exclude_none=True),\n",
    "                                        _record\n",
    "                                       ).description\n",
    "        # insert the generated image descriptions into the database\n",
    "        _genai_desc_record = insert_genai_description(_genai_desc)\n",
    "        # generate an embedding\n",
    "        _embedding = GeminiEmbedding(_description=_genai_desc.description, _record=_record).embedding\n",
    "        # insert embedding data into the database\n",
    "        _embedding_record = insert_embedding(_embedding)\n",
    "\n",
    "        return f\"Success! Asset with file name {_asset.file_name} has been updated in the database.\"\n",
    "        \n",
    "    elif not _update:\n",
    "        return (f\"Asset with file name {_asset.file_name} already exists in database. \"\n",
    "                \"If you would like to update the record for this asset, submit your request again with the update flag set to True.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af599b15-8099-4642-8895-a7de5aa259af",
   "metadata": {},
   "source": [
    "### Test out import workflow with a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a3b00d4-a0f5-464e-94b4-6a343f8eec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! Asset with file name 20211224_P8J8463.jpg has been updated in the database.\n"
     ]
    }
   ],
   "source": [
    "test_image_path = '/Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/20211224_P8J8463.jpg'\n",
    "\n",
    "result = import_image(test_image_path, _update=True)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4afc68-e5af-481e-9e0a-d6d55da6ed9e",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "Searching our image library is conversational. Our agent is going to help us find images in our library based on our query.\n",
    "\n",
    "Our agent has a host of tools at its disposal to process our query. Whatever our query, our agent will try its best to understand what we're looking for.\n",
    "\n",
    "Let's begin with a simple sql query tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff5c7654-bdcd-49fa-838c-89ed81ae8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sql search\n",
    "def search_image_library_sql(_sql_query: str) -> str:\n",
    "    \"\"\"\n",
    "    Search the image library database using sql queries.\n",
    "\n",
    "    Example: 'SELECT * FROM assets WHERE id == 1'\n",
    "\n",
    "    Args:\n",
    "        _sql_query (string): A sql command to execute on the image library database.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _query_results = db.q(_sql_query)\n",
    "\n",
    "    return _query_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10577cae-c43a-4e9b-8355-fff9b3cf58ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 7,\n",
       "  'image_path': '/Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/20211224_P8J8463.jpg',\n",
       "  'file_name': '20211224_P8J8463.jpg',\n",
       "  'file_type': None}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_image_library_sql(\"SELECT * FROM assets WHERE id == 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c8eb7-8d87-4eaf-885c-1698a05b70bf",
   "metadata": {},
   "source": [
    "### Topic Search\n",
    "\n",
    "Does our query include any keywords or phrases that exist in our taxonomy? Our agent will decide if it can extract any search terms that can be used to query our database. Generated summaries are used to better understand what we're asking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd12d5-0ead-4d44-8269-406f9ccc15ce",
   "metadata": {},
   "source": [
    "### Vector Search\n",
    "\n",
    "Our query is vectorized and used to search similarities in our database. The agent might do a few things to better produce an embedding of our query that matches embeddings in our database. It might ask itself if our query is a description of an image and whether or not it can be modified to better describe images in our database. Our agent may summarize our query and even expand on it. Perhaps our agent goes a step further and transforms our query into a prompt that could generate images of our query, then that prompt in vectorized and used in a similarity search. For now, we will define a simple nearest neighbors search based on the generated descriptions in our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "141957da-786e-42ad-9a79-dadcf267633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8392435d-b174-4991-82f8-a85e9508bc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neighbors:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.index_map = {}\n",
    "        self.train_model()\n",
    "\n",
    "    def train_model(self):\n",
    "        vectors_query = db.q(\"SELECT * FROM embeddings\")\n",
    "        self.index_map = {}\n",
    "        vectors = []\n",
    "        for i, v in enumerate(vectors_query):\n",
    "            self.index_map[i] = v['id']\n",
    "            vectors.append(np.frombuffer(v['genai_description_vector'], dtype=np.float32))\n",
    "\n",
    "        neighbors = NearestNeighbors(n_neighbors=5, radius=1.0)\n",
    "\n",
    "        self.model = neighbors.fit(np.array(vectors))\n",
    "\n",
    "    def search(self, search_vector):\n",
    "        search_result = self.model.radius_neighbors(search_vector.reshape(1, -1), sort_results=True)\n",
    "        return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d7da486-5853-4f4a-b18c-234f1c43d65e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;NearestNeighbors<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.neighbors.NearestNeighbors.html\">?<span>Documentation for NearestNeighbors</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>NearestNeighbors()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neighbors = Neighbors()\n",
    "neighbors.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "61944702-8b20-4403-8314-79686f89a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_image_library_semantic(_query_text: str) -> list[Assets]:\n",
    "    \"\"\"\n",
    "    Search the image library using vector search.\n",
    "    The query text should describe an image that we're looking for.\n",
    "    Accepts query as text, generates an embedding representation of the query,\n",
    "    and returns results from a nearest neighbors search.\n",
    "\n",
    "    Args:\n",
    "        _query (string): Query text string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"semantic query: {_query_text}\")\n",
    "    \n",
    "    # Generate an embedding of our query text\n",
    "    _response = client.models.embed_content(model=\"models/text-embedding-004\",\n",
    "                                            contents=_query_text)\n",
    "    # Format our embedding as a numpy array\n",
    "    _embedding = np.array(_response.embeddings[0].values).astype(np.float32)\n",
    "    # Query our nearest neighbors model\n",
    "    _results = neighbors.search(search_vector=_embedding)[1].tolist()[0]\n",
    "    \n",
    "    return [Assets(**search_image_library_sql(f\"SELECT * FROM assets WHERE id == {neighbors.index_map[_res]}\")[0]) for _res in _results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be5d802d-824d-42ab-9e44-9fa7bde8e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic query: Cactus plants\n",
      "Assets(id=3, image_path='/Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/20190224_Sandcastle_Cactus_0036.jpg', file_name='20190224_Sandcastle_Cactus_0036.jpg', file_type=None)\n",
      "Assets(id=6, image_path='/Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/2018-12-22_Peruvian_Torch_0468.jpg', file_name='2018-12-22_Peruvian_Torch_0468.jpg', file_type=None)\n"
     ]
    }
   ],
   "source": [
    "for res in search_image_library_semantic(\"Cactus plants\"):\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b531f001-1f3e-4fb5-86d8-bb85e7b8fa13",
   "metadata": {},
   "source": [
    "### Set up Function Calling Tools\n",
    "\n",
    "Declare functions that our chat model will have access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e81a210b-42f6-4d31-84f1-bb2bef2d9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import_image_func = types.FunctionDeclaration(\n",
    "    name='import_image',\n",
    "    description='Open an image, get information and metadata about the image, and import it into a database.',\n",
    "    parameters=types.Schema(\n",
    "        type=types.Type('OBJECT'),\n",
    "        properties={\n",
    "            \"_image_path\": types.Schema(type=types.Type('STRING')),\n",
    "            \"_update\": types.Schema(type=types.Type('BOOLEAN'))\n",
    "        })\n",
    ")\n",
    "\n",
    "search_image_library_sql_func = types.FunctionDeclaration(\n",
    "    name='search_image_library_sql',\n",
    "    description=(\"Search the image library sqlite database using sql queries. \"\n",
    "                 f\"Database schema: {db.schema}\"\n",
    "                ),\n",
    "    parameters=types.Schema(\n",
    "        type=types.Type('OBJECT'),\n",
    "        properties={\n",
    "            \"_sql_query\": types.Schema(type=types.Type('STRING'))\n",
    "        })\n",
    ")\n",
    "\n",
    "search_image_library_semantic_func = types.FunctionDeclaration(\n",
    "    name='search_image_library_semantic',\n",
    "    description=(\"Search the image library using vector search.\"\n",
    "                 \"The query text should describe an image that we're looking for. \"\n",
    "                 \"Accepts query as text, generates an embedding representation of the query, \"\n",
    "                 \"and returns results from a nearest neighbors search.\"\n",
    "                ),\n",
    "    parameters=types.Schema(\n",
    "        type=types.Type('OBJECT'),\n",
    "        properties={\n",
    "            \"_query_text\": types.Schema(type=types.Type('STRING'))\n",
    "        })\n",
    ")\n",
    "\n",
    "tools = types.Tool(function_declarations=[import_image_func,\n",
    "                                          search_image_library_sql_func,\n",
    "                                          search_image_library_semantic_func\n",
    "                                         ]\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d9ce2b-7a2c-4272-bbc5-4c1f0645a7de",
   "metadata": {},
   "source": [
    "### Start a chat\n",
    "\n",
    "We're ready to instruct our Gemini agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "303a505b-7264-4837-9c45-8ac7b8900db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_SYSTEM_INSTSTRUCTION = (\"You are my image library database administrator. \"\n",
    "                             \"I provide a path to an image and you import it into the database. \"\n",
    "                             \"I ask a question about the image library, you execute sql statements to query the database. \"\n",
    "                             \"I ask for an image report, you generate a report with an image's information. \"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "579db427-7375-4158-8aa5-3d911b70e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure our model\n",
    "CHAT_MODEL_CONFIG = types.GenerateContentConfig(safety_settings=None,\n",
    "                                           tools=[tools],\n",
    "                                           tool_config=types.ToolConfig(\n",
    "                                               function_calling_config=types.FunctionCallingConfig(\n",
    "                                                   mode=types.FunctionCallingConfigMode(\"AUTO\"))),\n",
    "                                           system_instruction=CHAT_SYSTEM_INSTSTRUCTION,\n",
    "                                           max_output_tokens=2048,\n",
    "                                           temperature=0.3,\n",
    "                                           top_p=0.6,\n",
    "                                           top_k=32,\n",
    "                                           presence_penalty=0.3,\n",
    "                                           frequency_penalty=0.3,\n",
    "                                           automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True,\n",
    "                                                                                                           maximum_remote_calls=None\n",
    "                                                                                                           )\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6a3e4e4-e0c1-4a11-9363-4c5be083453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a new chat\n",
    "chat = client.chats.create(model='gemini-2.0-flash-exp',\n",
    "                           config=CHAT_MODEL_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "73ab7416-3c7d-4879-91cb-de5dbee45402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our two-part prompt to begin our chat with\n",
    "prompt_text = types.Part.from_text(text=\"Hey Gemini! I have an image to import into my database. Do not update the record if it already exists. Please help!\")\n",
    "prompt_image = types.Part.from_text(text=test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac56ecbe-0682-41fc-b9ab-f68cb6e31ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send our message and get a response\n",
    "response = chat.send_message(message=[prompt_text, prompt_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72e1b9c2-c5aa-4378-b87f-a3863fe82a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=FunctionCall(id=None, args={'_update': False, '_image_path': '/Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/20211224_P8J8463.jpg'}, name='import_image'), function_response=None, inline_data=None, text=None)], role='model'), citation_metadata=None, finish_message=None, token_count=None, avg_logprobs=-0.001403958189721201, finish_reason=<FinishReason.STOP: 'STOP'>, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=[SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None), SafetyRating(blocked=None, category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>, probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>, probability_score=None, severity=None, severity_score=None)])], model_version='gemini-2.0-flash-exp', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cached_content_token_count=None, candidates_token_count=51, prompt_token_count=596, total_token_count=647), automatic_function_calling_history=None, parsed=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at our first response\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a86d4f88-8a93-4b38-ac5a-b4746e654ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_response(_response):\n",
    "    \n",
    "    if function_calls := _response.function_calls:\n",
    "        results = []\n",
    "        content = \"\"\n",
    "        for call in function_calls:\n",
    "            if call.name == 'import_image':\n",
    "                content = import_image(_image_path=call.args['_image_path'], _update=call.args['_update'])\n",
    "            elif call.name == 'search_image_library_sql':\n",
    "                content = search_image_library_sql(_sql_query=call.args['_sql_query'])\n",
    "            elif call.name == 'search_image_library_semantic':\n",
    "                content = search_image_library_semantic(_query_text=call.args['_query_text'])\n",
    "                \n",
    "            results.append(\n",
    "                types.Part.from_function_response(\n",
    "                    name=call.name,\n",
    "                    response={'content': content}\n",
    "                )\n",
    "            )\n",
    "\n",
    "        time.sleep(1)\n",
    "        return process_response(chat.send_message(results))\n",
    "\n",
    "    return _response.text\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fdf8f473-1082-4c27-ba2a-ff8e6eb3586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent]: OK. It looks like that image already exists in the database. I did not update the record.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[user]:  Thanks! Please update the record for me :-)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent]: OK. I've updated the record for you.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[user]:  Generate an image report with the image's information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent]: OK. Here's the image report:\n",
      "\n",
      "**File Path:** /Users/peterjakubowski/Desktop/Jupyter/google_genai/datasets/image_library/20211224_P8J8463.jpg\n",
      "**File Name:** 20211224_P8J8463.jpg\n",
      "**File Type:** None\n",
      "**Capture Date:** 2021-12-24T20:26:21.350000\n",
      "**Description:** A studio shot of a Yule Log cake on a white cake stand, photographed on December 24, 2021. The cake is a chocolate swiss roll with a creamy white filling, covered in chocolate frosting that has been textured to resemble tree bark. The cake is garnished with fresh rosemary sprigs, bright red cranberries, and sliced almonds. The cake is sitting on a bed of crushed chocolate cookies to resemble dirt. The background is a white tiled wall and a white surface.\n",
      "**Keywords:** Yule Log, cake, dessert, chocolate, frosting, rosemary, cranberries, almonds, cookies, Christmas, holiday, sweet, baked, treat, festive\n",
      "**Creator:** Peter Jakubowski\n",
      "**Person in Image:** None\n",
      "**Location:** None\n",
      "**City:** None\n",
      "**State:** None\n",
      "**Style:** Bright\n",
      "**Mood:** Joyful\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[user]:  Awesome!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[agent]: Is there anything else I can help you with?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[user]:  STOP\n"
     ]
    }
   ],
   "source": [
    "# Let's chat for a while\n",
    "do_some_work = True\n",
    "\n",
    "while do_some_work:\n",
    "            \n",
    "    print(f\"[agent]: {process_response(response)}\")\n",
    "\n",
    "    user_input = input(\"[user]: \")\n",
    "\n",
    "    if user_input == 'STOP':\n",
    "        do_some_work = False\n",
    "\n",
    "    else:\n",
    "        response = chat.send_message(message=user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e439f2d-74f5-4308-b783-f68a7e17f21b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
